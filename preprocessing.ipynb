{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d83f0b-835f-4dc6-ad98-2458a5ae41f8",
   "metadata": {},
   "source": [
    "# Data Preprocsesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194b204-97bd-4769-b0a4-63ae135a0da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ca95b-dcb3-4490-abff-c6df385eaedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open('data/trec_corpus_20220301_plain.json') as read_from_file:\n",
    "    with open('data/preprocessed_corputs.csv', 'a') as write_to_file:\n",
    "        \n",
    "        # write header of csv file\n",
    "        write_to_file.write('id, text')\n",
    "        doc = read_from_file.readline()\n",
    "\n",
    "        for i in range(5): # while doc:\n",
    "            \n",
    "            # parse json\n",
    "            doc_parsed = json.loads(doc)\n",
    "            doc_text = doc_parsed['title'] + ' ' + doc_parsed['plain']\n",
    "            \n",
    "            print(doc_parsed)\n",
    "            print('\\n\\n############\\n\\n')\n",
    "\n",
    "            # do preprocessing\n",
    "            preprocessed_text = 'test test test test test'\n",
    "\n",
    "            # write preprocessed document to file (append)\n",
    "            write_to_file.write(f'{doc_id}, {preprocessed_text}\\n')\n",
    "\n",
    "            # read next line\n",
    "            doc = read_from_file.readline()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae10a83-f8fa-462a-8a3d-52aab036c3f1",
   "metadata": {},
   "source": [
    "# Approach of Abda\n",
    "- Loading data\n",
    "- Preprocessing first 5 lines to save computation\n",
    "- Appending preprecessed data into a dictionary / dataframe \n",
    "- Saving finished data after preprocessing into csv or feather file for continuous analysis and evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "731b8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading necessary libraries\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "db0490c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>plain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12148915</td>\n",
       "      <td>Keith Osik</td>\n",
       "      <td>Keith Richard Osik (born October 22, 1968), is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16752449</td>\n",
       "      <td>Swansons Landing, Texas</td>\n",
       "      <td>Swansons Landing is a settlement and former in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31967453</td>\n",
       "      <td>Mike Potts</td>\n",
       "      <td>Mike or Michael Potts may refer to:\\n Michael ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47436994</td>\n",
       "      <td>Shuker</td>\n",
       "      <td>Shuker is a surname. Notable people with the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13924699</td>\n",
       "      <td>William Clark (inventor)</td>\n",
       "      <td>William Clark (17 March 1821 – 22 January 1880...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                     title  \\\n",
       "0  12148915                Keith Osik   \n",
       "1  16752449   Swansons Landing, Texas   \n",
       "2  31967453                Mike Potts   \n",
       "3  47436994                    Shuker   \n",
       "4  13924699  William Clark (inventor)   \n",
       "\n",
       "                                               plain  \n",
       "0  Keith Richard Osik (born October 22, 1968), is...  \n",
       "1  Swansons Landing is a settlement and former in...  \n",
       "2  Mike or Michael Potts may refer to:\\n Michael ...  \n",
       "3  Shuker is a surname. Notable people with the s...  \n",
       "4  William Clark (17 March 1821 – 22 January 1880...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Check first stats of first 10 rows\n",
      "------------------------------------\n",
      "<bound method Series.sort_values of id       0\n",
      "title    0\n",
      "plain    0\n",
      "dtype: int64>\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset -> Only first five rows\n",
    "df_prep = pd.read_json('data/trec_corpus_20220301_plain.json', lines=True, nrows = 100)\n",
    "\n",
    "# Dropping url-column\n",
    "df_prep.drop(columns=['url'], inplace=True, axis=1)\n",
    "display(df_prep.head())\n",
    "\n",
    "# Looking for missing / empty values\n",
    "print('\\nCheck first stats of first 10 rows')\n",
    "print('------------------------------------')\n",
    "print(df_prep.isna().sum().sort_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071bb996",
   "metadata": {},
   "source": [
    "## __Cleaning and preprocessing textual data__\n",
    "- Converting to lower case\n",
    "- How to handle missing data (No necessaty...?)\n",
    "- Removing punctuations\n",
    "- Removing stopwords\n",
    "- Tokenizing data (especially last column with plain text)\n",
    "- Normalizing data (Stemming / Lemmatization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2956896a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>plain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>15483958</td>\n",
       "      <td>Guignen</td>\n",
       "      <td>guignen (, gallo: ginyen) is a commune in the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4488874</td>\n",
       "      <td>Guy Park</td>\n",
       "      <td>guy park, also known as guy park state histori...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>55869679</td>\n",
       "      <td>Simone Rasmussen</td>\n",
       "      <td>simone rasmussen (born 8 may 1993) is a danish...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>61465636</td>\n",
       "      <td>Wright Creek (Nanticoke River tributary)</td>\n",
       "      <td>wright creek is a  long tributary to the nanti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>65041536</td>\n",
       "      <td>Manon De Roey</td>\n",
       "      <td>manon de roey (born 12 december 1991) is a pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                     title  \\\n",
       "24  15483958                                   Guignen   \n",
       "26   4488874                                  Guy Park   \n",
       "59  55869679                          Simone Rasmussen   \n",
       "32  61465636  Wright Creek (Nanticoke River tributary)   \n",
       "89  65041536                             Manon De Roey   \n",
       "\n",
       "                                                plain  \n",
       "24  guignen (, gallo: ginyen) is a commune in the ...  \n",
       "26  guy park, also known as guy park state histori...  \n",
       "59  simone rasmussen (born 8 may 1993) is a danish...  \n",
       "32  wright creek is a  long tributary to the nanti...  \n",
       "89  manon de roey (born 12 december 1991) is a pro...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========= Lower case whole column =========\n",
    "df_prep['plain'] = df_prep['plain'].str.lower()\n",
    "df_prep.sample(frac = 1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "79f2c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Removing punctuations =========\n",
    "import string\n",
    "def remove_punctuations(plain_text):\n",
    "    punctiations = string.punctuation\n",
    "    return plain_text.translate(str.maketrans('', '', punctiations))\n",
    "\n",
    "df_prep['plain'] = df_prep['plain'].apply(lambda x : remove_punctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4f0cce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Removing stopwords =========\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def remove_stopwords(plain_text):\n",
    "    return ' '.join([word for word in plain_text.split() if word not in STOPWORDS])\n",
    "\n",
    "df_prep['plain'] = df_prep['plain'].apply(lambda x: remove_stopwords(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a42e44b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Removing special characters =========\n",
    "import re\n",
    "\n",
    "def remove_spec_char(plain_text):\n",
    "    plain_text = re.sub('[^a-zA-Z0-9]', ' ', plain_text)\n",
    "    plain_text = re.sub('\\s+', ' ', plain_text)\n",
    "    return plain_text\n",
    "\n",
    "df_prep['plain'] = df_prep['plain'].apply(lambda x : remove_spec_char(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d8740a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Stemming =========\n",
    "# For the Stemming we create a separate series which will append the last feature of the dataframe\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(plain_text):\n",
    "    return ' '.join([ps.stem(word) for word in plain_text.split()])\n",
    "\n",
    "df_prep['plain_stemmed'] = df_prep['plain'].apply(lambda x : stem_words(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0a0e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/abderrahmanecharrade/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/abderrahmanecharrade/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ========= Lemmatization =========\n",
    "# For the Lemmatization we create a separate series which will append the last feature of the dataframe\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {'N':wordnet.NOUN, 'V':wordnet.VERB, 'J':wordnet.ADJ, 'R':wordnet.ADV}\n",
    "\n",
    "def lemmatize_word(plain_text):\n",
    "    # Finind pos tags\n",
    "    pos_text = pos_tag(plain_text.split())\n",
    "    return ' '.join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_text])\n",
    "\n",
    "df_prep['plain_lemmatized'] = df_prep['plain'].apply(lambda x : lemmatize_word(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285b8891",
   "metadata": {},
   "source": [
    "# __Finished preprocessing__\n",
    "If the preprocessing fulfills the requirements for further operations and building IR-Models,\n",
    "export finished dataframe as csv or [__feather__]('https://arrow.apache.org/docs/python/feather.html') file format (light-weighted option to csv) which saves in comparison to json and csv more computation power and cpu time<br></br>\n",
    "Next task is to build up the vector space model and starting with tf-idf / term-weighting\n",
    "\n",
    "----------------------------------------------------------------\n",
    "## __Writing in feather file format__\n",
    "import pyarrow.feather as feather<br>\n",
    "feather.write_feather(df, '/path/to/file')\n",
    "\n",
    "## __Writing in csv file format__\n",
    "DataFrame.to_csv('/path/to/file')<br></br>\n",
    "\n",
    "If the size of the finished and exported file is still too large, add it to .gitignorefile<br>\n",
    "Github for private use (non-commercial) does not allows too large repositories\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603bbf3a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit ('3.11.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "e3eb97f31843337c09a0fbe362ad0d1c57a4e5245c09f964828f772eab390a01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
