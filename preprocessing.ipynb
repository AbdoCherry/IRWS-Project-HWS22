{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d83f0b-835f-4dc6-ad98-2458a5ae41f8",
   "metadata": {},
   "source": [
    "# Data Preprocsesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194b204-97bd-4769-b0a4-63ae135a0da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ca95b-dcb3-4490-abff-c6df385eaedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "with open('data/trec_corpus_20220301_plain.json') as read_from_file:\n",
    "    with open('data/preprocessed_corputs.csv', 'a') as write_to_file:\n",
    "        \n",
    "        # write header of csv file\n",
    "        write_to_file.write('id, text')\n",
    "        doc = read_from_file.readline()\n",
    "\n",
    "        for i in range(5): # while doc:\n",
    "            \n",
    "            # parse json\n",
    "            doc_parsed = json.loads(doc)\n",
    "            doc_text = doc_parsed['title'] + ' ' + doc_parsed['plain']\n",
    "            \n",
    "            print(doc_parsed)\n",
    "            print('\\n\\n############\\n\\n')\n",
    "\n",
    "            # do preprocessing\n",
    "            preprocessed_text = 'test test test test test'\n",
    "\n",
    "            # write preprocessed document to file (append)\n",
    "            write_to_file.write(f'{doc_id}, {preprocessed_text}\\n')\n",
    "\n",
    "            # read next line\n",
    "            doc = read_from_file.readline()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae10a83-f8fa-462a-8a3d-52aab036c3f1",
   "metadata": {},
   "source": [
    "# Approach of Abda\n",
    "- Loading data\n",
    "- Preprocessing first 5 lines to save computation\n",
    "- Appending preprecessed data into a dictionary / dataframe \n",
    "- Saving finished data after preprocessing into csv or feather file for continuous analysis and evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "731b8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading necessary libraries\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db0490c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>plain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12148915</td>\n",
       "      <td>Keith Osik</td>\n",
       "      <td>Keith Richard Osik (born October 22, 1968), is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16752449</td>\n",
       "      <td>Swansons Landing, Texas</td>\n",
       "      <td>Swansons Landing is a settlement and former in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31967453</td>\n",
       "      <td>Mike Potts</td>\n",
       "      <td>Mike or Michael Potts may refer to:\\n Michael ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>47436994</td>\n",
       "      <td>Shuker</td>\n",
       "      <td>Shuker is a surname. Notable people with the s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13924699</td>\n",
       "      <td>William Clark (inventor)</td>\n",
       "      <td>William Clark (17 March 1821 – 22 January 1880...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                     title  \\\n",
       "0  12148915                Keith Osik   \n",
       "1  16752449   Swansons Landing, Texas   \n",
       "2  31967453                Mike Potts   \n",
       "3  47436994                    Shuker   \n",
       "4  13924699  William Clark (inventor)   \n",
       "\n",
       "                                               plain  \n",
       "0  Keith Richard Osik (born October 22, 1968), is...  \n",
       "1  Swansons Landing is a settlement and former in...  \n",
       "2  Mike or Michael Potts may refer to:\\n Michael ...  \n",
       "3  Shuker is a surname. Notable people with the s...  \n",
       "4  William Clark (17 March 1821 – 22 January 1880...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Check first stats of first 10 rows\n",
      "------------------------------------\n",
      "<bound method Series.sort_values of id       0\n",
      "title    0\n",
      "plain    0\n",
      "dtype: int64>\n"
     ]
    }
   ],
   "source": [
    "# Loading dataset -> Only first five rows\n",
    "df_prep = pd.read_json('/Users/abderrahmanecharrade/Desktop/Uni-Mannheim-Sonder/04_Semester/IRWS/Data/trec_corpus_20220301_plain.json', lines=True, nrows = 100)\n",
    "\n",
    "# Dropping url-column\n",
    "df_prep.drop(columns=['url'], inplace=True, axis=1)\n",
    "display(df_prep.head())\n",
    "\n",
    "# Looking for missing / empty values\n",
    "print('\\nCheck first stats of first 10 rows')\n",
    "print('------------------------------------')\n",
    "print(df_prep.isna().sum().sort_values)\n",
    "\n",
    "# Concatenating title column with plain-text column\n",
    "df_prep['plain-text'] = df_prep['title'].astype(str) + ' ' + df_prep['plain'].astype(str)\n",
    "df_prep.drop(['title', 'plain'], axis = 1, inplace = True)\n",
    "df_prep.rename(columns = {'plain-text' : 'plain'}, inplace = True)\n",
    "display(df_prep.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071bb996",
   "metadata": {},
   "source": [
    "## __Cleaning and preprocessing textual data__\n",
    "- Converting to lower case\n",
    "- How to handle missing data (No necessaty...?)\n",
    "- Removing punctuations\n",
    "- Removing stopwords\n",
    "- Tokenizing data (especially last column with plain text)\n",
    "- Normalizing data (Stemming / Lemmatization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2956896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Lower case whole column =========\n",
    "df_prep['plain'] = df_prep['plain'].str.lower()\n",
    "df_prep.sample(frac = 1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d89b05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prep['title'].sample(frac = 1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b3fdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Tokenization =========\n",
    "from nltk import word_tokenize\n",
    "test_doc = df_prep.iloc[0,2]\n",
    "test_doc_tokens = word_tokenize(test_doc)\n",
    "print(test_doc_tokens)\n",
    "\n",
    "def tokenize_words(plain_text):\n",
    "    tokenized_text = word_tokenize(plain_text)\n",
    "    return tokenized_text\n",
    "\n",
    "df_prep_tokenized = df_prep['plain'].apply(lambda x: tokenize_words(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f2c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Removing punctuations =========\n",
    "import string\n",
    "def remove_punctuations(plain_text):\n",
    "    punctiations = string.punctuation\n",
    "    return plain_text.translate(str.maketrans('', '', punctiations))\n",
    "\n",
    "df_prep['plain'] = df_prep['plain'].apply(lambda x : remove_punctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0cce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Removing stopwords =========\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def remove_stopwords(plain_text):\n",
    "    return ' '.join([word for word in plain_text.split() if word not in STOPWORDS])\n",
    "\n",
    "df_prep['plain'] = df_prep['plain'].apply(lambda x: remove_stopwords(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e44b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Removing special characters =========\n",
    "import re\n",
    "\n",
    "def remove_spec_char(plain_text):\n",
    "    plain_text = re.sub('[^a-zA-Z0-9]', ' ', plain_text)\n",
    "    plain_text = re.sub('\\s+', ' ', plain_text)\n",
    "    return plain_text\n",
    "\n",
    "df_prep['plain'] = df_prep['plain'].apply(lambda x : remove_spec_char(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8740a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ========= Stemming =========\n",
    "# For the Stemming we create a separate series which will append the last feature of the dataframe\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(plain_text):\n",
    "    return ' '.join([ps.stem(word) for word in plain_text.split()])\n",
    "\n",
    "df_prep['plain_stemmed'] = df_prep['plain'].apply(lambda x : stem_words(x))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0a0e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Lemmatization =========\n",
    "# For the Lemmatization we create a separate series which will append the last feature of the dataframe\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {'N':wordnet.NOUN, 'V':wordnet.VERB, 'J':wordnet.ADJ, 'R':wordnet.ADV}\n",
    "\n",
    "def lemmatize_word(plain_text):\n",
    "    # Finind pos tags\n",
    "    pos_text = pos_tag(plain_text.split())\n",
    "    return ' '.join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_text])\n",
    "\n",
    "df_prep['plain_lemmatized'] = df_prep['plain'].apply(lambda x : lemmatize_word(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267ca5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_prep.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c5c7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Stemmed')\n",
    "print(df_prep.iloc[0,3])\n",
    "print('\\nLemmatized')\n",
    "print(df_prep.iloc[0,4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285b8891",
   "metadata": {},
   "source": [
    "# __Finished preprocessing__\n",
    "If the preprocessing fulfills the requirements for further operations and building IR-Models,\n",
    "export finished dataframe as csv or [__feather__]('https://arrow.apache.org/docs/python/feather.html') file format (light-weighted option to csv) which saves in comparison to json and csv more computation power and cpu time<br></br>\n",
    "Next task is to build up the vector space model and starting with tf-idf / term-weighting\n",
    "\n",
    "----------------------------------------------------------------\n",
    "## __Writing in feather file format__\n",
    "import pyarrow.feather as feather<br>\n",
    "feather.write_feather(df, '/path/to/file')\n",
    "\n",
    "## __Writing in csv file format__\n",
    "DataFrame.to_csv('/path/to/file')<br></br>\n",
    "\n",
    "If the size of the finished and exported file is still too large, add it to .gitignorefile<br>\n",
    "Github for private use (non-commercial) does not allows too large repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603bbf3a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('irws-project': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f41925ae1994f5fc38598add9a8aab6b752964107725e1bd6b4144c42565ad7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
