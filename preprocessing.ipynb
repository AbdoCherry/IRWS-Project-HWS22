{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6d83f0b-835f-4dc6-ad98-2458a5ae41f8",
   "metadata": {},
   "source": [
    "# Data Preprocsesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4194b204-97bd-4769-b0a4-63ae135a0da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e92ca95b-dcb3-4490-abff-c6df385eaedf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('data/trec_corpus_20220301_plain.json') as read_from_file:\\n    with open('data/preprocessed_corputs.csv', 'a') as write_to_file:\\n        \\n        # write header of csv file\\n        write_to_file.write('id, text')\\n        doc = read_from_file.readline()\\n\\n        for i in range(5): # while doc:\\n            \\n            # parse json\\n            doc_parsed = json.loads(doc)\\n            doc_text = doc_parsed['title'] + ' ' + doc_parsed['plain']\\n            \\n            print(doc_parsed)\\n            print('\\n\\n############\\n\\n')\\n\\n            # do preprocessing\\n            preprocessed_text = 'test test test test test'\\n\\n            # write preprocessed document to file (append)\\n            write_to_file.write(f'{doc_id}, {preprocessed_text}\\n')\\n\\n            # read next line\\n            doc = read_from_file.readline()\\n\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "with open('data/trec_corpus_20220301_plain.json') as read_from_file:\n",
    "    with open('data/preprocessed_corputs.csv', 'a') as write_to_file:\n",
    "        \n",
    "        # write header of csv file\n",
    "        write_to_file.write('id, text')\n",
    "        doc = read_from_file.readline()\n",
    "\n",
    "        for i in range(5): # while doc:\n",
    "            \n",
    "            # parse json\n",
    "            doc_parsed = json.loads(doc)\n",
    "            doc_text = doc_parsed['title'] + ' ' + doc_parsed['plain']\n",
    "            \n",
    "            print(doc_parsed)\n",
    "            print('\\n\\n############\\n\\n')\n",
    "\n",
    "            # do preprocessing\n",
    "            preprocessed_text = 'test test test test test'\n",
    "\n",
    "            # write preprocessed document to file (append)\n",
    "            write_to_file.write(f'{doc_id}, {preprocessed_text}\\n')\n",
    "\n",
    "            # read next line\n",
    "            doc = read_from_file.readline()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae10a83-f8fa-462a-8a3d-52aab036c3f1",
   "metadata": {},
   "source": [
    "# Approach of Abda\n",
    "- Loading data\n",
    "- Preprocessing first 5 lines to save computation\n",
    "- Appending preprecessed data into a dictionary / dataframe \n",
    "- Saving finished data after preprocessing into csv or feather file for continuous analysis and evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "731b8692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading necessary libraries\n",
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "db0490c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading dataset -> Only first five rows\n",
    "df_prep = pd.read_json('/Users/abderrahmanecharrade/Desktop/Uni-Mannheim-Sonder/04_Semester/IRWS/Data/trec_corpus_20220301_plain.json', lines=True)\n",
    "\n",
    "# Dropping url-column\n",
    "df_prep.drop(columns=['url'], inplace=True, axis=1)\n",
    "display(df_prep.head())\n",
    "\n",
    "# Looking for missing / empty values\n",
    "print('\\nCheck first stats of first 10 rows')\n",
    "print('------------------------------------')\n",
    "print(df_prep.isna().sum().sort_values)\n",
    "\n",
    "# Concatenating title column with plain-text column\n",
    "df_prep['plain-text'] = df_prep['title'].astype(str) + ' ' + df_prep['plain'].astype(str)\n",
    "df_prep.drop(['title', 'plain'], axis = 1, inplace = True)\n",
    "df_prep.rename(columns = {'plain-text' : 'plain'}, inplace = True)\n",
    "display(df_prep.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071bb996",
   "metadata": {},
   "source": [
    "## __Cleaning and preprocessing textual data__\n",
    "- Converting to lower case\n",
    "- How to handle missing data (No necessaty...?)\n",
    "- Removing punctuations\n",
    "- Removing stopwords\n",
    "- Tokenizing data (especially last column with plain text)\n",
    "- Normalizing data (Stemming / Lemmatization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2956896a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>plain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>49647788</td>\n",
       "      <td>białe augustowskie (lake) białe augustowskie (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>56457702</td>\n",
       "      <td>franca ronchese franca ronchese is an italian-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>24522576</td>\n",
       "      <td>irving corey second lieutenant irving banfield...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>65057452</td>\n",
       "      <td>1896 tipperary senior hurling championship the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13924699</td>\n",
       "      <td>william clark (inventor) william clark (17 mar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              plain\n",
       "98  49647788  białe augustowskie (lake) białe augustowskie (...\n",
       "29  56457702  franca ronchese franca ronchese is an italian-...\n",
       "46  24522576  irving corey second lieutenant irving banfield...\n",
       "38  65057452  1896 tipperary senior hurling championship the...\n",
       "4   13924699  william clark (inventor) william clark (17 mar..."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ========= Lower case whole column =========\n",
    "df_prep['plain'] = df_prep['plain'].str.lower()\n",
    "df_prep.sample(frac = 1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d4b3fdf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['keith', 'osik', 'keith', 'richard', 'osik', '(', 'born', 'october', '22', ',', '1968', ')', ',', 'is', 'a', 'former', 'major', 'league', 'baseball', 'catcher', 'who', 'played', 'in', 'the', 'major', 'leagues', 'from', '-', '.', 'he', 'played', 'for', 'the', 'milwaukee', 'brewers', ',', 'pittsburgh', 'pirates', ',', 'baltimore', 'orioles', ',', 'and', 'washington', 'nationals', '.', 'he', 'was', 'drafted', 'in', 'the', '24th', 'round', 'of', 'the', 'mlb', 'draft', '.', 'his', 'brother', 'was', 'also', 'a', 'professional', 'baseball', 'player', ',', 'but', 'only', 'played', 'in', 'the', 'minors', '.', 'he', 'was', 'born', 'in', 'port', 'washington', ',', 'new', 'york', ',', 'but', 'now', 'lives', 'in', 'shoreham', ',', 'new', 'york', '.', 'osik', 'is', 'currently', 'the', 'head', 'baseball', 'coach', 'at', 'farmingdale', 'state', 'college', ',', 'a', 'division', 'iii', 'institution', 'located', 'on', 'long', 'island', 'in', 'new', 'york', '.', 'he', 'was', 'inducted', 'into', 'the', 'suffolk', 'sports', 'hall', 'of', 'fame', 'on', 'long', 'island', 'in', 'the', 'baseball', 'category', 'with', 'the', 'class', 'of', '2008.', 'external', 'links', '1968', 'births', 'living', 'people', 'major', 'league', 'baseball', 'catchers', 'baseball', 'players', 'from', 'new', 'york', '(', 'state', ')', 'people', 'from', 'port', 'washington', ',', 'new', 'york', 'milwaukee', 'brewers', 'players', 'pittsburgh', 'pirates', 'players', 'baltimore', 'orioles', 'players', 'washington', 'nationals', 'players', 'buffalo', 'bisons', '(', 'minor', 'league', ')', 'players', 'nashville', 'sounds', 'players', 'durham', 'bulls', 'players', 'albuquerque', 'isotopes', 'players', 'new', 'orleans', 'zephyrs', 'players', 'farmingdale', 'state', 'rams', 'baseball', 'coaches']\n"
     ]
    }
   ],
   "source": [
    "# ========= Tokenization =========\n",
    "from nltk import word_tokenize\n",
    "test_doc = df_prep.iloc[0,1]\n",
    "test_doc_tokens = word_tokenize(test_doc)\n",
    "print(test_doc_tokens)\n",
    "\n",
    "def tokenize_words(plain_text):\n",
    "    tokenized_text = word_tokenize(plain_text)\n",
    "    return tokenized_text\n",
    "\n",
    "df_prep_tokenized = df_prep['plain'].apply(lambda x: tokenize_words(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79f2c654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Removing punctuations =========\n",
    "import string\n",
    "def remove_punctuations(plain_text):\n",
    "    punctiations = string.punctuation\n",
    "    return plain_text.translate(str.maketrans('', '', punctiations))\n",
    "\n",
    "df_prep['plain'] = df_prep['plain'].apply(lambda x : remove_punctuations(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f0cce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Removing stopwords =========\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def remove_stopwords(plain_text):\n",
    "    return ' '.join([word for word in plain_text.split() if word not in STOPWORDS])\n",
    "\n",
    "df_prep['plain'] = df_prep['plain'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a42e44b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Removing special characters =========\n",
    "import re\n",
    "\n",
    "def remove_spec_char(plain_text):\n",
    "    plain_text = re.sub('[^a-zA-Z0-9]', ' ', plain_text)\n",
    "    plain_text = re.sub('\\s+', ' ', plain_text)\n",
    "    return plain_text\n",
    "\n",
    "df_prep['plain'] = df_prep['plain'].apply(lambda x : remove_spec_char(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8740a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# ========= Stemming =========\n",
    "# For the Stemming we create a separate series which will append the last feature of the dataframe\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stem_words(plain_text):\n",
    "    return ' '.join([ps.stem(word) for word in plain_text.split()])\n",
    "\n",
    "df_prep['plain_stemmed'] = df_prep['plain'].apply(lambda x : stem_words(x))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e8d242",
   "metadata": {},
   "source": [
    "### Lemmatization instead of Stemming:\n",
    "> \"Stemming is a faster process than lemmatization as stemming chops off the word irrespective of the context, whereas the latter is context-dependent. Stemming is a rule-based approach, whereas lemmatization is a canonical dictionary-based approach. Lemmatization has higher accuracy than stemming.\"<br></br>\n",
    "     [__Explained Lemmatization vs. Stemming__](\"https://analyticsindiamag.com/explained-stemming-vs-lemmatization-in-nlp/\")<br></br>\n",
    "We have decided to move further with Lemmatization, because some words inspected by sampling were not representing the context and meaning of possibly queried terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fd0a0e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= Lemmatization =========\n",
    "# For the Lemmatization we create a separate series which will append the last feature of the dataframe\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "# Comment two lines below out if not packages 'wordnet' & 'omw-1.4' not available \n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {'N':wordnet.NOUN, 'V':wordnet.VERB, 'J':wordnet.ADJ, 'R':wordnet.ADV}\n",
    "\n",
    "def lemmatize_word(plain_text):\n",
    "    # Finind pos tags\n",
    "    pos_text = pos_tag(plain_text.split())\n",
    "    return ' '.join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_text])\n",
    "\n",
    "df_prep['plain_lemmatized'] = df_prep['plain'].apply(lambda x : lemmatize_word(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285b8891",
   "metadata": {},
   "source": [
    "# __Finished preprocessing__\n",
    "If the preprocessing fulfills the requirements for further operations and building IR-Models,\n",
    "export finished dataframe as csv or [__feather__]('https://arrow.apache.org/docs/python/feather.html') file format (light-weighted option to csv) which saves in comparison to json and csv more computation power and cpu time<br></br>\n",
    "Next task is to build up the vector space model and starting with tf-idf / term-weighting\n",
    "\n",
    "----------------------------------------------------------------\n",
    "## __Writing in feather file format__\n",
    "import pyarrow.feather as feather<br>\n",
    "feather.write_feather(df, '/path/to/file')\n",
    "\n",
    "## __Writing in csv file format__\n",
    "DataFrame.to_csv('/path/to/file')<br></br>\n",
    "\n",
    "If the size of the finished and exported file is still too large, add it to .gitignorefile<br>\n",
    "Github for private use (non-commercial) does not allows too large repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603bbf3a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 ('irws-project': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "f41925ae1994f5fc38598add9a8aab6b752964107725e1bd6b4144c42565ad7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
